{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics\n",
        "from ultralytics import YOLO\n",
        "\n",
        "path_to_pytorch_model = 'yolov8n-pose.pt'\n",
        "model = YOLO(path_to_pytorch_model)\n",
        "\n",
        "model.export(format='onnx', imgsz=640, opset=11)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "n0wrZuWEgAdI",
        "outputId": "e9bba6a2-6295-472b-8cfd-351df871f0be"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.28-py3-none-any.whl.metadata (35 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.8.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (10.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.11-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.28-py3-none-any.whl (881 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m881.2/881.2 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.11-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.28 ultralytics-thop-2.0.11\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n-pose.pt to 'yolov8n-pose.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6.52M/6.52M [00:00<00:00, 26.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.28 🚀 Python-3.10.12 torch-2.5.0+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "YOLOv8n-pose summary (fused): 187 layers, 3,289,964 parameters, 0 gradients, 9.2 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolov8n-pose.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 56, 8400) (6.5 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 11...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.37...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 3.9s, saved as 'yolov8n-pose.onnx' (12.8 MB)\n",
            "\n",
            "Export complete (8.2s)\n",
            "Results saved to \u001b[1m/content\u001b[0m\n",
            "Predict:         yolo predict task=pose model=yolov8n-pose.onnx imgsz=640  \n",
            "Validate:        yolo val task=pose model=yolov8n-pose.onnx imgsz=640 data=/usr/src/app/ultralytics/datasets/coco-pose.yaml  \n",
            "Visualize:       https://netron.app\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'yolov8n-pose.onnx'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class YOLOv8Pose():\n",
        "    def __init__(self, model_path, nc = 1, num_of_keypoints = 17):\n",
        "        self.session = ort.InferenceSession(model_path)\n",
        "        self.input_details  = [i for i in self.session.get_inputs()]\n",
        "        self.output_details = [i.name for i in self.session.get_outputs()]\n",
        "        self.X_axis = [0, 2] + [5 + i * 3 for i in range(num_of_keypoints)]\n",
        "        self.y_axis = [1, 3] + [6 + i * 3 for i in range(num_of_keypoints)]\n",
        "        self.nc = nc\n",
        "\n",
        "    def predict(self, frames, conf=0.25, iou=0.7, agnostic=False, max_det=300):\n",
        "        im = self.preprocess(frames)\n",
        "        preds = self.inference(im)\n",
        "        results = self.postprocess(preds, conf_thres=conf, iou_thres=iou, agnostic=agnostic, max_det=max_det)\n",
        "        return results\n",
        "\n",
        "    def postprocess(self, preds, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False, multi_label=False, labels=(), max_det=300, nc=0, max_time_img=0.05, max_nms=30000, max_wh=7680, in_place=True, rotated=False):\n",
        "        xc = np.max(preds[:, 4: self.nc + 4], axis = 1) > conf_thres\n",
        "        preds = np.transpose(preds, (0, 2, 1))\n",
        "        preds[..., :4] = xywh2xyxy(preds[..., :4])\n",
        "        x = preds[0][xc[0]]\n",
        "\n",
        "        if not x.shape[0]:\n",
        "          return None\n",
        "        box, cls, keypoints = x[:, :4], x[:, 4:5], x[:, 5:]\n",
        "        j = np.argmax(cls, axis=1)\n",
        "        conf = cls[[i for i in range(len(j))], j]\n",
        "        concatenated = np.concatenate((box, conf.reshape(-1, 1), j.reshape(-1, 1).astype(float), keypoints), axis=1)\n",
        "        x = concatenated[conf.flatten() > conf_thres]\n",
        "\n",
        "        if x.shape[0] > max_nms:  # excess boxes\n",
        "            x = x[x[:, 4].argsort(descending=True)[:max_nms]]\n",
        "        cls = x[:, 5:6] * (0 if agnostic else max_wh)\n",
        "        scores, boxes = x[:, 4], x[:, :4] + cls\n",
        "\n",
        "        i = non_max_suppression(boxes, scores, iou_thres)\n",
        "        return [x[i[:max_det]]]\n",
        "\n",
        "    def inference(self, im):\n",
        "        inputs = {key.name: value for key, value in zip(self.input_details, [im])}\n",
        "        preds = self.session.run(self.output_details, inputs)[0]\n",
        "        preds[:, self.X_axis] *= im.shape[2] ; preds[:, self.y_axis] *= im.shape[3]\n",
        "        return preds\n",
        "\n",
        "    def preprocess(self, im):\n",
        "        im = np.stack(self.pre_transform(im))\n",
        "        im = im[..., ::-1]\n",
        "        im = np.ascontiguousarray(im).astype(np.float32)\n",
        "        im /= 255.0\n",
        "        im = np.transpose(im, (0, 3, 1, 2))\n",
        "        return im\n",
        "\n",
        "    def pre_transform(self, im):\n",
        "        imgsz = self.input_details[0].shape[2:4]\n",
        "        return [cv2.resize(im[0], imgsz, interpolation=cv2.INTER_LINEAR) for x in im]\n",
        "\n",
        "class LetterBox:\n",
        "    def __init__(self, new_shape=(640, 640), auto=False, scaleFill=False, scaleup=True, center=True, stride=32):\n",
        "        self.new_shape = new_shape\n",
        "        self.auto = auto\n",
        "        self.scaleFill = scaleFill\n",
        "        self.scaleup = scaleup\n",
        "        self.stride = stride\n",
        "        self.center = center\n",
        "\n",
        "    def __call__(self, labels=None, image=None):\n",
        "        if labels is None:\n",
        "            labels = {}\n",
        "        img = labels.get(\"img\") if image is None else image\n",
        "        shape = img.shape[:2]\n",
        "        new_shape = labels.pop(\"rect_shape\", self.new_shape)\n",
        "        if isinstance(new_shape, int):\n",
        "            new_shape = (new_shape, new_shape)\n",
        "\n",
        "        r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "        if not self.scaleup:\n",
        "            r = min(r, 1.0)\n",
        "\n",
        "        ratio = r, r\n",
        "        new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "        dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "        if self.auto:\n",
        "            dw, dh = np.mod(dw, self.stride), np.mod(dh, self.stride)  # wh padding\n",
        "        elif self.scaleFill:\n",
        "            dw, dh = 0.0, 0.0\n",
        "            new_unpad = (new_shape[1], new_shape[0])\n",
        "            ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
        "\n",
        "        if self.center:\n",
        "            dw /= 2\n",
        "            dh /= 2\n",
        "\n",
        "        if shape[::-1] != new_unpad:\n",
        "            img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "        top, bottom = int(round(dh - 0.1)) if self.center else 0, int(round(dh + 0.1))\n",
        "        left, right = int(round(dw - 0.1)) if self.center else 0, int(round(dw + 0.1))\n",
        "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(114, 114, 114))\n",
        "        if labels.get(\"ratio_pad\"):\n",
        "            labels[\"ratio_pad\"] = (labels[\"ratio_pad\"], (left, top))  # for evaluation\n",
        "\n",
        "        if len(labels):\n",
        "            labels = self._update_labels(labels, ratio, dw, dh)\n",
        "            labels[\"img\"] = img\n",
        "            labels[\"resized_shape\"] = new_shape\n",
        "            return labels\n",
        "        else:\n",
        "            return img\n",
        "\n",
        "    def _update_labels(self, labels, ratio, padw, padh):\n",
        "        labels[\"instances\"].convert_bbox(format=\"xyxy\")\n",
        "        labels[\"instances\"].denormalize(*labels[\"img\"].shape[:2][::-1])\n",
        "        labels[\"instances\"].scale(*ratio)\n",
        "        labels[\"instances\"].add_padding(padw, padh)\n",
        "        return labels\n",
        "\n",
        "def xywh2xyxy(x):\n",
        "    assert x.shape[-1] == 4, f\"input shape last dimension expected 4 but input shape is {x.shape}\"\n",
        "    y = np.empty_like(x)\n",
        "    xy = x[..., :2]\n",
        "    wh = x[..., 2:] / 2\n",
        "    y[..., :2] = xy - wh\n",
        "    y[..., 2:] = xy + wh\n",
        "    return y\n",
        "\n",
        "def non_max_suppression(boxes, scores, iou_threshold):\n",
        "    x1 = boxes[:, 0]\n",
        "    y1 = boxes[:, 1]\n",
        "    x2 = boxes[:, 2]\n",
        "    y2 = boxes[:, 3]\n",
        "\n",
        "    areas = (x2 - x1) * (y2 - y1)\n",
        "    order = scores.argsort()[::-1]\n",
        "\n",
        "    keep = []\n",
        "    while order.size > 0:\n",
        "        i = order[0]\n",
        "        keep.append(i)\n",
        "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
        "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
        "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
        "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
        "        w = np.maximum(0.0, xx2 - xx1)\n",
        "        h = np.maximum(0.0, yy2 - yy1)\n",
        "        inter = w * h\n",
        "        iou = inter / (areas[i] + areas[order[1:]] - inter)\n",
        "        inds = np.where(iou <= iou_threshold)[0]\n",
        "        order = order[inds + 1]\n",
        "\n",
        "    return np.array(keep)\n",
        "\n",
        "def plot(image, results, connections, visible = 0.1):\n",
        "    for bboxes in results:\n",
        "      x1, y1, x2, y2 = int(bboxes[0]), int(bboxes[1]), int(bboxes[2]), int(bboxes[3])\n",
        "      conf, cls = bboxes[4] , bboxes[5]\n",
        "      cv2.rectangle(image, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=3)\n",
        "      cv2.putText(image, f'instance {conf:.2f}', (x1, y1 - 2), 0, 1, [0, 255, 0], thickness=2, lineType=cv2.LINE_AA)\n",
        "\n",
        "      keypoints = bboxes[6:].reshape(-1, 3)\n",
        "      for i in range(len(keypoints)):\n",
        "        if keypoints[i][2] > visible:\n",
        "          cv2.circle(image, (int(keypoints[i][0]), int(keypoints[i][1])), 5, (0, 255, 0), -1)\n",
        "\n",
        "      for connection in connections:\n",
        "        start_idx, end_idx = connection\n",
        "        if start_idx < len(keypoints) and end_idx < len(keypoints):\n",
        "          if keypoints[start_idx][2] > visible and keypoints[end_idx][2] > visible:\n",
        "            cv2.line(image, (int(keypoints[start_idx][0]), int(keypoints[start_idx][1])), (int(keypoints[end_idx][0]), int(keypoints[end_idx][1])), (0, 255, 0), 2)\n",
        "    return image"
      ],
      "metadata": {
        "id": "bxoukUHjcaSq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime as ort\n",
        "\n",
        "connections = [(4, 2), (2, 0), (3, 1), (1, 0), (0, 6), (6, 8), (8, 10),\n",
        "               (0, 5), (5, 7), (7, 9), (0, 12), (12, 14), (14, 16), (0, 11), (11, 13), (13, 15)]\n",
        "\n",
        "onnx_model = YOLOv8Pose(model_path='./yolov8n-pose.onnx', num_of_keypoints = 17)\n",
        "results = onnx_model.predict([frame])\n",
        "plot(frame.copy(), results[0].copy(), connections)"
      ],
      "metadata": {
        "id": "s99rScYrAfwR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}