{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZTwwSjCE8ys"
      },
      "source": [
        "# Load the Configuration of YOLOs\n",
        "\n",
        "<img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\" alt=\"Running on Colab\" width='60'> [Runing on colab]()\n",
        "\n",
        "**1. Import model from ultralytics framework (PyTorch)** :\n",
        "\n",
        "Load the pre-trained YOLO model from the ultralytics library. This library provides development tools for the YOLO (You Only Look Once) series of computer vision models. You are allowed to select the currently validated Pose model in `model_name`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx onnxconverter-common"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "K5ny7Yr6y1YI",
        "outputId": "d3ba0df5-5ca6-4d99-ffdc-9852d20b235a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting onnxconverter-common\n",
            "  Downloading onnxconverter_common-1.14.0-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (4.25.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxconverter-common) (24.2)\n",
            "Collecting protobuf>=3.20.2 (from onnx)\n",
            "  Downloading protobuf-3.20.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
            "Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxconverter_common-1.14.0-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.20.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf, onnx, onnxconverter-common\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.5\n",
            "    Uninstalling protobuf-4.25.5:\n",
            "      Successfully uninstalled protobuf-4.25.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.62.3 requires protobuf>=4.21.6, but you have protobuf 3.20.2 which is incompatible.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed onnx-1.17.0 onnxconverter-common-1.14.0 protobuf-3.20.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "a4ce39c1d3de4856a9e65a4e9062af29"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMjHRw8CVQYu",
        "outputId": "1d939e4a-4811-4275-cc87-822435df478b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.39-py3-none-any.whl.metadata (35 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.8.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (11.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.12-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.39-py3-none-any.whl (896 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m896.9/896.9 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.12-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.39 ultralytics-thop-2.0.12\n",
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.35M/5.35M [00:00<00:00, 57.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model_name = \"yolo11n.pt\"   # @param [\"yolov5nu.pt\", \"yolov5su.pt\", \"yolov5mu.pt\", \"yolov5lu.pt\", \"yolov5xu.pt\", \"yolov8n.pt\", \"yolov8s.pt\", \"yolov8m.pt\", \"yolov8l.pt\", \"yolov8x.pt\", \"yolo11n.pt\", \"yolo11s.pt\", \"yolo11m.pt\", \"yolo11l.pt\", \"yolo11x.pt\"]\n",
        "model = YOLO(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWc4DxZ7G1Jz"
      },
      "source": [
        "# Delegation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozr-DhWiNCQi"
      },
      "source": [
        "## Export to ONNX\n",
        "Usage: [[KleidiAI]](https://github.com/R300-AI/ITRI-AI-Hub/tree/main/Model-Zoo/Object-Detection/YOLOs/KleidiAI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VsOGn3vMFFO"
      },
      "source": [
        "**Step1.** Export the model to ONNX format with FP32 and FP16."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "id": "hmmT3fcLJ9ke",
        "outputId": "62d6d202-93b8-481c-b625-340ad603713b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.39 🚀 Python-3.10.12 torch-2.5.1+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "YOLO11n summary (fused): 238 layers, 2,616,248 parameters, 0 gradients, 6.5 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo11n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (5.4 MB)\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['onnxslim', 'onnxruntime'] not found, attempting AutoUpdate...\n",
            "Collecting onnxslim\n",
            "  Downloading onnxslim-0.1.42-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (from onnxslim) (1.17.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxslim) (1.13.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxslim) (24.2)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.3.25)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.26.4)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (3.20.2)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxslim) (1.3.0)\n",
            "Downloading onnxslim-0.1.42-py3-none-any.whl (142 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 142.7/142.7 kB 10.2 MB/s eta 0:00:00\n",
            "Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.3/13.3 MB 51.7 MB/s eta 0:00:00\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 46.0/46.0 kB 101.7 MB/s eta 0:00:00\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.8/86.8 kB 117.4 MB/s eta 0:00:00\n",
            "Installing collected packages: humanfriendly, onnxslim, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.20.1 onnxslim-0.1.42\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 12.9s, installed 2 packages: ['onnxslim', 'onnxruntime']\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 12...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.42...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 20.2s, saved as 'yolo11n.onnx' (10.2 MB)\n",
            "\n",
            "Export complete (22.6s)\n",
            "Results saved to \u001b[1m/content\u001b[0m\n",
            "Predict:         yolo predict task=detect model=yolo11n.onnx imgsz=640  \n",
            "Validate:        yolo val task=detect model=yolo11n.onnx imgsz=640 data=/usr/src/ultralytics/ultralytics/cfg/datasets/coco.yaml  \n",
            "Visualize:       https://netron.app\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'yolo11n.onnx'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "output_path = model.export(format='onnx', imgsz=640, opset=12)\n",
        "output_path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from onnxconverter_common import float16\n",
        "import os, onnx\n",
        "\n",
        "model = onnx.load(output_path)\n",
        "model_fp16 = float16.convert_float_to_float16(model)\n",
        "onnx.save(model_fp16, output_path.replace('.onnx', '_float16.onnx'))\n",
        "os.rename(output_path, output_path.replace('.onnx', '_float32.onnx'))"
      ],
      "metadata": {
        "id": "zesMl_Liy92j"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edXkGMKmMF1x"
      },
      "source": [
        "**Step2.** Validate the accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mR-D0jEOMKY_",
        "outputId": "9065dc7b-4cd0-4a35-ff79-aefca032e73f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING ⚠️ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
            "Ultralytics 8.3.39 🚀 Python-3.10.12 torch-2.5.1+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "Loading yolo11n_fp32.onnx for ONNX Runtime inference...\n",
            "Preferring ONNX Runtime AzureExecutionProvider\n",
            "Setting batch=1 input of shape (1, 3, 640, 640)\n",
            "\n",
            "Dataset 'coco8.yaml' images not found ⚠️, missing path '/content/datasets/coco8/images/val'\n",
            "Downloading https://ultralytics.com/assets/coco8.zip to '/content/datasets/coco8.zip'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 433k/433k [00:00<00:00, 9.76MB/s]\n",
            "Unzipping /content/datasets/coco8.zip to /content/datasets/coco8...: 100%|██████████| 25/25 [00:00<00:00, 1400.34file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset download success ✅ (0.6s), saved to \u001b[1m/content/datasets\u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 755k/755k [00:00<00:00, 13.4MB/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco8/labels/val... 4 images, 0 backgrounds, 0 corrupt: 100%|██████████| 4/4 [00:00<00:00, 65.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/coco8/labels/val.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  3.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.846      0.637       0.85      0.608\n",
            "                person          3         10          1      0.324      0.603      0.293\n",
            "                   dog          1          1      0.724          1      0.995      0.697\n",
            "                 horse          1          2      0.964          1      0.995      0.462\n",
            "              elephant          1          2      0.672        0.5      0.518      0.307\n",
            "              umbrella          1          1      0.719          1      0.995      0.995\n",
            "          potted plant          1          1          1          0      0.995      0.895\n",
            "Speed: 20.0ms preprocess, 221.7ms inference, 0.0ms loss, 8.1ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val\u001b[0m\n",
            "0.6082262589758726\n",
            "0.850260337085477\n"
          ]
        }
      ],
      "source": [
        "delegated_model = YOLO(output_path.replace('.onnx', '_float32.onnx'))\n",
        "metrics = delegated_model.val(data=\"coco8.yaml\")\n",
        "\n",
        "print(metrics.box.map)  # mAP50-95\n",
        "print(metrics.box.map50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NXyPT3rMNP4"
      },
      "source": [
        "**Step3.** Reproduce the output process manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYQcXZuUFqzq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class YOLOs():\n",
        "    def __init__(self, model_path, nc = 1, num_of_keypoints = 17):\n",
        "        self.session = ort.InferenceSession(model_path)\n",
        "        self.input_details  = [i for i in self.session.get_inputs()]\n",
        "        self.output_details = [i.name for i in self.session.get_outputs()]\n",
        "        self.io = self.session.io_binding()\n",
        "        self.io.bind_output(self.output_details[0])\n",
        "\n",
        "        self.X_axis = [0, 2] + [5 + i * 3 for i in range(num_of_keypoints)]\n",
        "        self.y_axis = [1, 3] + [6 + i * 3 for i in range(num_of_keypoints)]\n",
        "        self.nc = nc\n",
        "\n",
        "    def predict(self, frames, conf=0.25, iou=0.7, agnostic=False, max_det=300):\n",
        "        im = self.preprocess(frames)\n",
        "        preds = self.inference(im)\n",
        "        results = self.postprocess(preds, conf_thres=conf, iou_thres=iou, agnostic=agnostic, max_det=max_det)\n",
        "        return results\n",
        "\n",
        "    def postprocess(self, preds, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False, multi_label=False, labels=(), max_det=300, nc=0, max_time_img=0.05, max_nms=30000, max_wh=7680, in_place=True, rotated=False):\n",
        "        xc = np.max(preds[:, 4: self.nc + 4], axis = 1) > conf_thres\n",
        "        preds = np.transpose(preds, (0, 2, 1))\n",
        "        preds[..., :4] = xywh2xyxy(preds[..., :4])\n",
        "        x = preds[0][xc[0]]\n",
        "\n",
        "        if not x.shape[0]:\n",
        "          return None\n",
        "        box, cls, keypoints = x[:, :4], x[:, 4:5], x[:, 5:]\n",
        "        j = np.argmax(cls, axis=1)\n",
        "        conf = cls[[i for i in range(len(j))], j]\n",
        "        concatenated = np.concatenate((box, conf.reshape(-1, 1), j.reshape(-1, 1).astype(float), keypoints), axis=1)\n",
        "        x = concatenated[conf.flatten() > conf_thres]\n",
        "\n",
        "        if x.shape[0] > max_nms:  # excess boxes\n",
        "            x = x[x[:, 4].argsort(descending=True)[:max_nms]]\n",
        "        cls = x[:, 5:6] * (0 if agnostic else max_wh)\n",
        "        scores, boxes = x[:, 4], x[:, :4] + cls\n",
        "\n",
        "        i = non_max_suppression(boxes, scores, iou_thres)\n",
        "        return [x[i[:max_det]]]\n",
        "\n",
        "    def inference(self, im):\n",
        "        self.io.bind_cpu_input(self.input_details[0].name, im)\n",
        "        self.session.run_with_iobinding(self.io)\n",
        "        return self.io.copy_outputs_to_cpu()[0]\n",
        "\n",
        "    def preprocess(self, im):\n",
        "        im = np.stack(self.pre_transform(im))\n",
        "        im = im[..., ::-1]\n",
        "        im = np.ascontiguousarray(im).astype(np.float32)\n",
        "        im /= 255.0\n",
        "        im = np.transpose(im, (0, 3, 1, 2))\n",
        "        return im\n",
        "\n",
        "    def pre_transform(self, im):\n",
        "        imgsz = self.input_details[0].shape[2:]\n",
        "        return [cv2.resize(im[0], imgsz, interpolation=cv2.INTER_LINEAR) for x in im]\n",
        "\n",
        "class LetterBox:\n",
        "    def __init__(self, new_shape=(640, 640), auto=False, scaleFill=False, scaleup=True, center=True, stride=32):\n",
        "        self.new_shape = new_shape\n",
        "        self.auto = auto\n",
        "        self.scaleFill = scaleFill\n",
        "        self.scaleup = scaleup\n",
        "        self.stride = stride\n",
        "        self.center = center\n",
        "\n",
        "    def __call__(self, labels=None, image=None):\n",
        "        if labels is None:\n",
        "            labels = {}\n",
        "        img = labels.get(\"img\") if image is None else image\n",
        "        shape = img.shape[:2]\n",
        "        new_shape = labels.pop(\"rect_shape\", self.new_shape)\n",
        "        if isinstance(new_shape, int):\n",
        "            new_shape = (new_shape, new_shape)\n",
        "\n",
        "        r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "        if not self.scaleup:\n",
        "            r = min(r, 1.0)\n",
        "\n",
        "        ratio = r, r\n",
        "        new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "        dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "        if self.auto:\n",
        "            dw, dh = np.mod(dw, self.stride), np.mod(dh, self.stride)  # wh padding\n",
        "        elif self.scaleFill:\n",
        "            dw, dh = 0.0, 0.0\n",
        "            new_unpad = (new_shape[1], new_shape[0])\n",
        "            ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
        "\n",
        "        if self.center:\n",
        "            dw /= 2\n",
        "            dh /= 2\n",
        "\n",
        "        if shape[::-1] != new_unpad:\n",
        "            img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "        top, bottom = int(round(dh - 0.1)) if self.center else 0, int(round(dh + 0.1))\n",
        "        left, right = int(round(dw - 0.1)) if self.center else 0, int(round(dw + 0.1))\n",
        "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(114, 114, 114))\n",
        "        if labels.get(\"ratio_pad\"):\n",
        "            labels[\"ratio_pad\"] = (labels[\"ratio_pad\"], (left, top))  # for evaluation\n",
        "\n",
        "        if len(labels):\n",
        "            labels = self._update_labels(labels, ratio, dw, dh)\n",
        "            labels[\"img\"] = img\n",
        "            labels[\"resized_shape\"] = new_shape\n",
        "            return labels\n",
        "        else:\n",
        "            return img\n",
        "\n",
        "    def _update_labels(self, labels, ratio, padw, padh):\n",
        "        labels[\"instances\"].convert_bbox(format=\"xyxy\")\n",
        "        labels[\"instances\"].denormalize(*labels[\"img\"].shape[:2][::-1])\n",
        "        labels[\"instances\"].scale(*ratio)\n",
        "        labels[\"instances\"].add_padding(padw, padh)\n",
        "        return labels\n",
        "\n",
        "def plot(image, results, labels):\n",
        "    for bboxes in results:\n",
        "      x1, y1, x2, y2 = int(bboxes[0] * image.shape[1]), int(bboxes[1] * image.shape[0]), int(bboxes[2] * image.shape[1]), int(bboxes[3] * image.shape[0])\n",
        "      conf, cls = bboxes[4] , bboxes[5]\n",
        "      cv2.rectangle(image, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=3)\n",
        "      cv2.putText(image, f'{labels[int(cls)]} {conf:.2f}', (x1, y1 - 2), 0, 1, [0, 255, 0], thickness=2, lineType=cv2.LINE_AA)\n",
        "    return image\n",
        "\n",
        "def xywh2xyxy(x):\n",
        "    assert x.shape[-1] == 4, f\"input shape last dimension expected 4 but input shape is {x.shape}\"\n",
        "    y = np.empty_like(x)\n",
        "    xy = x[..., :2]\n",
        "    wh = x[..., 2:] / 2\n",
        "    y[..., :2] = xy - wh\n",
        "    y[..., 2:] = xy + wh\n",
        "    return y\n",
        "\n",
        "def non_max_suppression(boxes, scores, iou_threshold):\n",
        "    x1 = boxes[:, 0]\n",
        "    y1 = boxes[:, 1]\n",
        "    x2 = boxes[:, 2]\n",
        "    y2 = boxes[:, 3]\n",
        "\n",
        "    areas = (x2 - x1) * (y2 - y1)\n",
        "    order = scores.argsort()[::-1]\n",
        "\n",
        "    keep = []\n",
        "    while order.size > 0:\n",
        "        i = order[0]\n",
        "        keep.append(i)\n",
        "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
        "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
        "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
        "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
        "        w = np.maximum(0.0, xx2 - xx1)\n",
        "        h = np.maximum(0.0, yy2 - yy1)\n",
        "        inter = w * h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEoVNbrVJvr2"
      },
      "outputs": [],
      "source": [
        "!wget https://ultralytics.com/images/bus.jpg -O bus.jpg\n",
        "import onnxruntime as ort\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "labels = {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant',\n",
        "      11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear',\n",
        "      22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball',\n",
        "      33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork',\n",
        "      43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut',\n",
        "      55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard',\n",
        "      67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear',\n",
        "      78: 'hair drier', 79: 'toothbrush'}\n",
        "\n",
        "img = cv2.imread('./bus.jpg')\n",
        "onnx_model = YOLOs(model_path=output_path)\n",
        "#results = onnx_model.predict([img], conf=0.25, iou=0.7, agnostic=False, max_det=300)\n",
        "\n",
        "#plt.imshow(plot(img.copy(), results[0].copy(), labels)[:, :, ::-1])\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVa9YwR7FrLb"
      },
      "source": [
        "## Export to TFLite\n",
        "Usage: [[None]]()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akOrJ7s0HXOY"
      },
      "source": [
        "**Step1.** Export the model to TFLite format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNLPPe-rHVD9"
      },
      "outputs": [],
      "source": [
        "output_path = model.export(format='tflite', imgsz=640)\n",
        "output_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZU3t_juHX80"
      },
      "source": [
        "**Step2.** Validate the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvebGwiZHYET"
      },
      "outputs": [],
      "source": [
        "delegated_model = YOLO(output_path)\n",
        "metrics = delegated_model.val(data=\"coco8.yaml\")\n",
        "\n",
        "print(metrics.box.map)  # mAP50-95\n",
        "print(metrics.box.map50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWklgbxIISNf"
      },
      "outputs": [],
      "source": [
        "delegated_model = YOLO(output_path.replace('float32', 'float16'))\n",
        "metrics = delegated_model.val(data=\"coco8.yaml\")\n",
        "\n",
        "print(metrics.box.map)  # mAP50-95\n",
        "print(metrics.box.map50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EByhTWAYIl_c"
      },
      "source": [
        "**Step3.** Reproduce the output process manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPPwRZQrVmIQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class YOLOs():\n",
        "    def __init__(self, model_path, nc = 1):\n",
        "        self.interpreter = tf_lite.Interpreter(model_path=model_path)\n",
        "        self.input_details = self.interpreter.get_input_details()\n",
        "        self.output_details = self.interpreter.get_output_details()\n",
        "        self.interpreter.allocate_tensors()\n",
        "        self.X_axis = [0, 2]\n",
        "        self.y_axis = [1, 3]\n",
        "        self.nc = nc\n",
        "\n",
        "    def predict(self, frames, conf=0.25, iou=0.7, agnostic=False, max_det=300):\n",
        "        im = self.preprocess(frames)\n",
        "        preds = self.inference(im)\n",
        "        results = self.postprocess(preds, conf_thres=conf, iou_thres=iou, agnostic=agnostic, max_det=max_det)\n",
        "        return results\n",
        "\n",
        "    def postprocess(self, preds, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False, multi_label=False, labels=(), max_det=300, nc=0, max_time_img=0.05, max_nms=30000, max_wh=7680, in_place=True, rotated=False):\n",
        "        xc = np.max(preds[:, 4: self.nc + 4], axis = 1) > conf_thres\n",
        "        preds = np.transpose(preds, (0, 2, 1))\n",
        "        preds[..., :4] = xywh2xyxy(preds[..., :4])\n",
        "        x = preds[0][xc[0]]\n",
        "\n",
        "        if not x.shape[0]:\n",
        "          return None\n",
        "        box, cls, keypoints = x[:, :4], x[:, 4:5], x[:, 5:]\n",
        "        j = np.argmax(cls, axis=1)\n",
        "        conf = cls[[i for i in range(len(j))], j]\n",
        "        concatenated = np.concatenate((box, conf.reshape(-1, 1), j.reshape(-1, 1).astype(float), keypoints), axis=1)\n",
        "        x = concatenated[conf.flatten() > conf_thres]\n",
        "\n",
        "        if x.shape[0] > max_nms:  # excess boxes\n",
        "            x = x[x[:, 4].argsort(descending=True)[:max_nms]]\n",
        "        cls = x[:, 5:6] * (0 if agnostic else max_wh)\n",
        "        scores, boxes = x[:, 4], x[:, :4] + cls\n",
        "\n",
        "        i = non_max_suppression(boxes, scores, iou_thres)\n",
        "        return [x[i[:max_det]]]\n",
        "\n",
        "    def inference(self, im):\n",
        "        self.interpreter.set_tensor(self.input_details[0]['index'], im)\n",
        "        self.interpreter.invoke()\n",
        "        preds = self.interpreter.get_tensor(self.output_details[0]['index'])\n",
        "        preds[:, self.X_axis] *= im.shape[1] ; preds[:, self.y_axis] *= im.shape[2]\n",
        "        return preds\n",
        "\n",
        "    def preprocess(self, im):\n",
        "        im = np.stack(self.pre_transform(im))\n",
        "        im = im[..., ::-1]\n",
        "        im = np.ascontiguousarray(im).astype(np.float32)\n",
        "        im /= 255.0\n",
        "        return im\n",
        "\n",
        "    def pre_transform(self, im):\n",
        "        imgsz = self.input_details[0]['shape'][1:3]\n",
        "        return [cv2.resize(im[0], imgsz, interpolation=cv2.INTER_LINEAR) for x in im]\n",
        "\n",
        "class LetterBox:\n",
        "    def __init__(self, new_shape=(640, 640), auto=False, scaleFill=False, scaleup=True, center=True, stride=32):\n",
        "        self.new_shape = new_shape\n",
        "        self.auto = auto\n",
        "        self.scaleFill = scaleFill\n",
        "        self.scaleup = scaleup\n",
        "        self.stride = stride\n",
        "        self.center = center\n",
        "\n",
        "    def __call__(self, labels=None, image=None):\n",
        "        if labels is None:\n",
        "            labels = {}\n",
        "        img = labels.get(\"img\") if image is None else image\n",
        "        shape = img.shape[:2]\n",
        "        new_shape = labels.pop(\"rect_shape\", self.new_shape)\n",
        "        if isinstance(new_shape, int):\n",
        "            new_shape = (new_shape, new_shape)\n",
        "\n",
        "        r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "        if not self.scaleup:\n",
        "            r = min(r, 1.0)\n",
        "\n",
        "        ratio = r, r\n",
        "        new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "        dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "        if self.auto:\n",
        "            dw, dh = np.mod(dw, self.stride), np.mod(dh, self.stride)  # wh padding\n",
        "        elif self.scaleFill:\n",
        "            dw, dh = 0.0, 0.0\n",
        "            new_unpad = (new_shape[1], new_shape[0])\n",
        "            ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
        "\n",
        "        if self.center:\n",
        "            dw /= 2\n",
        "            dh /= 2\n",
        "\n",
        "        if shape[::-1] != new_unpad:\n",
        "            img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "        top, bottom = int(round(dh - 0.1)) if self.center else 0, int(round(dh + 0.1))\n",
        "        left, right = int(round(dw - 0.1)) if self.center else 0, int(round(dw + 0.1))\n",
        "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(114, 114, 114))\n",
        "        if labels.get(\"ratio_pad\"):\n",
        "            labels[\"ratio_pad\"] = (labels[\"ratio_pad\"], (left, top))  # for evaluation\n",
        "\n",
        "        if len(labels):\n",
        "            labels = self._update_labels(labels, ratio, dw, dh)\n",
        "            labels[\"img\"] = img\n",
        "            labels[\"resized_shape\"] = new_shape\n",
        "            return labels\n",
        "        else:\n",
        "            return img\n",
        "\n",
        "    def _update_labels(self, labels, ratio, padw, padh):\n",
        "        labels[\"instances\"].convert_bbox(format=\"xyxy\")\n",
        "        labels[\"instances\"].denormalize(*labels[\"img\"].shape[:2][::-1])\n",
        "        labels[\"instances\"].scale(*ratio)\n",
        "        labels[\"instances\"].add_padding(padw, padh)\n",
        "        return labels\n",
        "\n",
        "def plot(image, results, labels):\n",
        "    for bboxes in results:\n",
        "      x1, y1, x2, y2 = int(bboxes[0] * image.shape[1] / 640), int(bboxes[1] * image.shape[0] / 640), int(bboxes[2] * image.shape[1] / 640), int(bboxes[3] * image.shape[0] / 640)\n",
        "      conf, cls = bboxes[4] , bboxes[5]\n",
        "      cv2.rectangle(image, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=3)\n",
        "      cv2.putText(image, f'{labels[int(cls)]} {conf:.2f}', (x1, y1 - 2), 0, 1, [0, 255, 0], thickness=2, lineType=cv2.LINE_AA)\n",
        "    return image\n",
        "\n",
        "def xywh2xyxy(x):\n",
        "    assert x.shape[-1] == 4, f\"input shape last dimension expected 4 but input shape is {x.shape}\"\n",
        "    y = np.empty_like(x)\n",
        "    xy = x[..., :2]\n",
        "    wh = x[..., 2:] / 2\n",
        "    y[..., :2] = xy - wh\n",
        "    y[..., 2:] = xy + wh\n",
        "    return y\n",
        "\n",
        "def non_max_suppression(boxes, scores, iou_threshold):\n",
        "    x1 = boxes[:, 0]\n",
        "    y1 = boxes[:, 1]\n",
        "    x2 = boxes[:, 2]\n",
        "    y2 = boxes[:, 3]\n",
        "\n",
        "    areas = (x2 - x1) * (y2 - y1)\n",
        "    order = scores.argsort()[::-1]\n",
        "\n",
        "    keep = []\n",
        "    while order.size > 0:\n",
        "        i = order[0]\n",
        "        keep.append(i)\n",
        "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
        "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
        "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
        "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
        "        w = np.maximum(0.0, xx2 - xx1)\n",
        "        h = np.maximum(0.0, yy2 - yy1)\n",
        "        inter = w * h\n",
        "        iou = inter / (areas[i] + areas[order[1:]] - inter)\n",
        "        inds = np.where(iou <= iou_threshold)[0]\n",
        "        order = order[inds + 1]\n",
        "\n",
        "    return np.array(keep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzvYF0QlVo14"
      },
      "outputs": [],
      "source": [
        "!wget https://ultralytics.com/images/bus.jpg -O bus.jpg\n",
        "import tensorflow.lite as tf_lite\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "labels = {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant',\n",
        "      11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear',\n",
        "      22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball',\n",
        "      33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork',\n",
        "      43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut',\n",
        "      55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard',\n",
        "      67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear',\n",
        "      78: 'hair drier', 79: 'toothbrush'}\n",
        "\n",
        "img = cv2.imread('./bus.jpg')\n",
        "tflite_model = YOLOs(model_path=output_path)\n",
        "results = tflite_model.predict([img], conf=0.25, iou=0.7, agnostic=False, max_det=300)\n",
        "\n",
        "plt.imshow(plot(img.copy(), results[0].copy(), labels)[:, :, ::-1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Or428eYcSQ8p"
      },
      "source": [
        "## Export to TorchScript\n",
        "Usage: [[None]]()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esTfPy7WSSys"
      },
      "source": [
        "**Step1.** Export the model to TorchScript format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pU2T_0OSfVp"
      },
      "outputs": [],
      "source": [
        "output_path = model.export(format='torchscript')\n",
        "output_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFTckSl-SzHN"
      },
      "source": [
        "**Step2.** Validate the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmZwjGSoSylk"
      },
      "outputs": [],
      "source": [
        "delegated_model = YOLO(output_path)\n",
        "metrics = delegated_model.val(data=\"coco8.yaml\")\n",
        "\n",
        "print(metrics.box.map)  # mAP50-95\n",
        "print(metrics.box.map50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVUaHhKmSfma"
      },
      "source": [
        "**Step3.** Reproduce the output process manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvSQwS8ESfho"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class YOLOs():\n",
        "    def __init__(self, model_path, nc = 1, imgsz=(640, 640)):\n",
        "        self.torch_model = torch.jit.load(model_path)\n",
        "        self.X_axis = [0, 2]\n",
        "        self.y_axis = [1, 3]\n",
        "        self.nc = nc\n",
        "        self.imgsz = imgsz\n",
        "\n",
        "    def predict(self, frames, conf=0.25, iou=0.7, agnostic=False, max_det=300):\n",
        "        im = self.preprocess(frames)\n",
        "        preds = self.inference(im)\n",
        "\n",
        "        results = self.postprocess(preds, conf_thres=conf, iou_thres=iou, agnostic=agnostic, max_det=max_det)\n",
        "        return results\n",
        "\n",
        "    def postprocess(self, preds, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False, multi_label=False, labels=(), max_det=300, nc=0, max_time_img=0.05, max_nms=30000, max_wh=7680, in_place=True, rotated=False):\n",
        "        xc = np.max(preds[:, 4: self.nc + 4], axis = 1) > conf_thres\n",
        "        preds = np.transpose(preds, (0, 2, 1))\n",
        "        preds[..., :4] = xywh2xyxy(preds[..., :4])\n",
        "        x = preds[0][xc[0]]\n",
        "\n",
        "        if not x.shape[0]:\n",
        "          return None\n",
        "        box, cls, keypoints = x[:, :4], x[:, 4:5], x[:, 5:]\n",
        "        j = np.argmax(cls, axis=1)\n",
        "        conf = cls[[i for i in range(len(j))], j]\n",
        "        concatenated = np.concatenate((box, conf.reshape(-1, 1), j.reshape(-1, 1).astype(float), keypoints), axis=1)\n",
        "        x = concatenated[conf.flatten() > conf_thres]\n",
        "\n",
        "        if x.shape[0] > max_nms:  # excess boxes\n",
        "            x = x[x[:, 4].argsort(descending=True)[:max_nms]]\n",
        "        cls = x[:, 5:6] * (0 if agnostic else max_wh)\n",
        "        scores, boxes = x[:, 4], x[:, :4] + cls\n",
        "\n",
        "        i = non_max_suppression(boxes, scores, iou_thres)\n",
        "        return [x[i[:max_det]]]\n",
        "\n",
        "    def inference(self, im):\n",
        "        with torch.no_grad():\n",
        "          im = torch.from_numpy(im)\n",
        "          preds = self.torch_model(im).numpy()\n",
        "        return preds\n",
        "\n",
        "    def preprocess(self, im):\n",
        "        im = np.stack(self.pre_transform(im))\n",
        "        im = im[..., ::-1]\n",
        "        im = np.ascontiguousarray(im).astype(np.float32)\n",
        "        im /= 255.0\n",
        "        im = np.transpose(im, (0, 3, 1, 2))\n",
        "        return im\n",
        "\n",
        "    def pre_transform(self, im):\n",
        "        return [cv2.resize(im[0], self.imgsz, interpolation=cv2.INTER_LINEAR) for x in im]\n",
        "\n",
        "class LetterBox:\n",
        "    def __init__(self, new_shape=(640, 640), auto=False, scaleFill=False, scaleup=True, center=True, stride=32):\n",
        "        self.new_shape = new_shape\n",
        "        self.auto = auto\n",
        "        self.scaleFill = scaleFill\n",
        "        self.scaleup = scaleup\n",
        "        self.stride = stride\n",
        "        self.center = center\n",
        "\n",
        "    def __call__(self, labels=None, image=None):\n",
        "        if labels is None:\n",
        "            labels = {}\n",
        "        img = labels.get(\"img\") if image is None else image\n",
        "        shape = img.shape[:2]\n",
        "        new_shape = labels.pop(\"rect_shape\", self.new_shape)\n",
        "        if isinstance(new_shape, int):\n",
        "            new_shape = (new_shape, new_shape)\n",
        "\n",
        "        r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "        if not self.scaleup:\n",
        "            r = min(r, 1.0)\n",
        "\n",
        "        ratio = r, r\n",
        "        new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "        dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "        if self.auto:\n",
        "            dw, dh = np.mod(dw, self.stride), np.mod(dh, self.stride)  # wh padding\n",
        "        elif self.scaleFill:\n",
        "            dw, dh = 0.0, 0.0\n",
        "            new_unpad = (new_shape[1], new_shape[0])\n",
        "            ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
        "\n",
        "        if self.center:\n",
        "            dw /= 2\n",
        "            dh /= 2\n",
        "\n",
        "        if shape[::-1] != new_unpad:\n",
        "            img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "        top, bottom = int(round(dh - 0.1)) if self.center else 0, int(round(dh + 0.1))\n",
        "        left, right = int(round(dw - 0.1)) if self.center else 0, int(round(dw + 0.1))\n",
        "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(114, 114, 114))\n",
        "        if labels.get(\"ratio_pad\"):\n",
        "            labels[\"ratio_pad\"] = (labels[\"ratio_pad\"], (left, top))  # for evaluation\n",
        "\n",
        "        if len(labels):\n",
        "            labels = self._update_labels(labels, ratio, dw, dh)\n",
        "            labels[\"img\"] = img\n",
        "            labels[\"resized_shape\"] = new_shape\n",
        "            return labels\n",
        "        else:\n",
        "            return img\n",
        "\n",
        "    def _update_labels(self, labels, ratio, padw, padh):\n",
        "        labels[\"instances\"].convert_bbox(format=\"xyxy\")\n",
        "        labels[\"instances\"].denormalize(*labels[\"img\"].shape[:2][::-1])\n",
        "        labels[\"instances\"].scale(*ratio)\n",
        "        labels[\"instances\"].add_padding(padw, padh)\n",
        "        return labels\n",
        "\n",
        "def plot(image, results, labels):\n",
        "    for bboxes in results:\n",
        "      x1, y1, x2, y2 = int(bboxes[0] * image.shape[1] / 640), int(bboxes[1] * image.shape[0] / 640), int(bboxes[2] * image.shape[1] / 640), int(bboxes[3] * image.shape[0] / 640)\n",
        "      conf, cls = bboxes[4] , bboxes[5]\n",
        "      cv2.rectangle(image, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=3)\n",
        "      cv2.putText(image, f'{labels[int(cls)]} {conf:.2f}', (x1, y1 - 2), 0, 1, [0, 255, 0], thickness=2, lineType=cv2.LINE_AA)\n",
        "    return image\n",
        "\n",
        "def xywh2xyxy(x):\n",
        "    assert x.shape[-1] == 4, f\"input shape last dimension expected 4 but input shape is {x.shape}\"\n",
        "    y = np.empty_like(x)\n",
        "    xy = x[..., :2]\n",
        "    wh = x[..., 2:] / 2\n",
        "    y[..., :2] = xy - wh\n",
        "    y[..., 2:] = xy + wh\n",
        "    return y\n",
        "\n",
        "def non_max_suppression(boxes, scores, iou_threshold):\n",
        "    x1 = boxes[:, 0]\n",
        "    y1 = boxes[:, 1]\n",
        "    x2 = boxes[:, 2]\n",
        "    y2 = boxes[:, 3]\n",
        "\n",
        "    areas = (x2 - x1) * (y2 - y1)\n",
        "    order = scores.argsort()[::-1]\n",
        "\n",
        "    keep = []\n",
        "    while order.size > 0:\n",
        "        i = order[0]\n",
        "        keep.append(i)\n",
        "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
        "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
        "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
        "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
        "        w = np.maximum(0.0, xx2 - xx1)\n",
        "        h = np.maximum(0.0, yy2 - yy1)\n",
        "        inter = w * h\n",
        "        iou = inter / (areas[i] + areas[order[1:]] - inter)\n",
        "        inds = np.where(iou <= iou_threshold)[0]\n",
        "        order = order[inds + 1]\n",
        "\n",
        "    return np.array(keep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGyNPhbjSfr8"
      },
      "outputs": [],
      "source": [
        "!wget https://ultralytics.com/images/bus.jpg -O bus.jpg\n",
        "import torch, cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "labels = {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant',\n",
        "      11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear',\n",
        "      22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball',\n",
        "      33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork',\n",
        "      43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut',\n",
        "      55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard',\n",
        "      67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear',\n",
        "      78: 'hair drier', 79: 'toothbrush'}\n",
        "\n",
        "img = cv2.imread('./bus.jpg')\n",
        "torch_model = YOLOs(model_path=output_path, imgsz=(640, 640))\n",
        "results = torch_model.predict([img], conf=0.25, iou=0.7, agnostic=False, max_det=300)\n",
        "plt.imshow(plot(img.copy(), results[0].copy(), labels)[:, :, ::-1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qY227Lw3UDyo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}