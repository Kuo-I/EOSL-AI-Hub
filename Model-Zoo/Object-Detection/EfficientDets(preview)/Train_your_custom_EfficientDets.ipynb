{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF9iZOxRouso"
      },
      "source": [
        "# Export EfficientDets via AutoML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8QsoStgoust"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/tensorflow/tensorflow/lite/g3doc/models/modify/model_maker/object_detection.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Run in Azure ML Studio</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCwzIBzyn5VX"
      },
      "source": [
        "### Step1. Build Environment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7DEUNO_orr1"
      },
      "source": [
        "> * **Ubuntu Server** and **Python3.8** Compiler is required for this script. We also recommand that you can use conda to build an virtual environment before running jupyter notebook.\n",
        ">   \n",
        ">   ```bash\n",
        ">   conda create --name automl python==3.8\n",
        ">   source activate automl\n",
        ">   ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "u3h45KEanTMo",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f0edcfa2-6558-47ed-ebc9-b5bad9034f9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -andas (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: tensorflow-model-optimization==0.7.0 in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: numpy~=1.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization==0.7.0) (1.21.6)\n",
            "Requirement already satisfied: six~=1.10 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization==0.7.0) (1.16.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization==0.7.0) (0.1.8)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -andas (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: tensorflowjs==3.18.0 in /usr/local/lib/python3.10/dist-packages (3.18.0)\n",
            "Requirement already satisfied: tensorflow<3,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs==3.18.0) (2.17.0)\n",
            "Requirement already satisfied: six<2,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs==3.18.0) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-hub<0.13,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs==3.18.0) (0.12.0)\n",
            "Requirement already satisfied: packaging~=20.9 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs==3.18.0) (20.9)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from packaging~=20.9->tensorflowjs==3.18.0) (3.2.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs==3.18.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs==3.18.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs==3.18.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs==3.18.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs==3.18.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs==3.18.0) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs==3.18.0) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs==3.18.0) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs==3.18.0) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs==3.18.0) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs==3.18.0) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs==3.18.0) (75.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs==3.18.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs==3.18.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs==3.18.0) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs==3.18.0) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs==3.18.0) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs==3.18.0) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs==3.18.0) (0.37.1)\n",
            "Collecting numpy<2.0.0,>=1.23.5 (from tensorflow<3,>=2.1.0->tensorflowjs==3.18.0)\n",
            "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<3,>=2.1.0->tensorflowjs==3.18.0) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<3,>=2.1.0->tensorflowjs==3.18.0) (13.9.3)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<3,>=2.1.0->tensorflowjs==3.18.0) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<3,>=2.1.0->tensorflowjs==3.18.0) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.1.0->tensorflowjs==3.18.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.1.0->tensorflowjs==3.18.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.1.0->tensorflowjs==3.18.0) (1.21.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.1.0->tensorflowjs==3.18.0) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<3,>=2.1.0->tensorflowjs==3.18.0) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<3,>=2.1.0->tensorflowjs==3.18.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<3,>=2.1.0->tensorflowjs==3.18.0) (3.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow<3,>=2.1.0->tensorflowjs==3.18.0) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow<3,>=2.1.0->tensorflowjs==3.18.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow<3,>=2.1.0->tensorflowjs==3.18.0) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow<3,>=2.1.0->tensorflowjs==3.18.0) (0.1.2)\n",
            "Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -andas (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "arviz 0.20.0 requires pandas>=1.5.0, but you have pandas 1.4.1 which is incompatible.\n",
            "bigframes 1.25.0 requires pandas>=1.5.3, but you have pandas 1.4.1 which is incompatible.\n",
            "cudf-cu12 24.10.1 requires numba>=0.57, but you have numba 0.55.0 which is incompatible.\n",
            "cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 1.4.1 which is incompatible.\n",
            "ibis-framework 9.2.0 requires pandas<3,>=1.5.3, but you have pandas 1.4.1 which is incompatible.\n",
            "mizani 0.13.0 requires pandas>=2.2.0, but you have pandas 1.4.1 which is incompatible.\n",
            "numba 0.55.0 requires numpy<1.22,>=1.18, but you have numpy 1.26.4 which is incompatible.\n",
            "pandas-gbq 0.24.0 requires packaging>=22.0.0, but you have packaging 20.9 which is incompatible.\n",
            "plotnine 0.14.0 requires pandas>=2.2.0, but you have pandas 1.4.1 which is incompatible.\n",
            "rmm-cu12 24.10.0 requires numba>=0.57, but you have numba 0.55.0 which is incompatible.\n",
            "scikit-image 0.24.0 requires packaging>=21, but you have packaging 20.9 which is incompatible.\n",
            "shap 0.46.0 requires packaging>20.9, but you have packaging 20.9 which is incompatible.\n",
            "statsmodels 0.14.4 requires packaging>=21.3, but you have packaging 20.9 which is incompatible.\n",
            "xarray 2024.10.0 requires packaging>=23.1, but you have packaging 20.9 which is incompatible.\n",
            "xarray 2024.10.0 requires pandas>=2.1, but you have pandas 1.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -andas (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: twine==1.9.0 in /usr/local/lib/python3.10/dist-packages (1.9.0)\n",
            "Requirement already satisfied: tqdm>=4.11 in /usr/local/lib/python3.10/dist-packages (from twine==1.9.0) (4.66.6)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from twine==1.9.0) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from twine==1.9.0) (1.0.0)\n",
            "Requirement already satisfied: pkginfo>=1.0 in /usr/local/lib/python3.10/dist-packages (from twine==1.9.0) (1.11.2)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from twine==1.9.0) (75.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->twine==1.9.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->twine==1.9.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->twine==1.9.0) (1.21.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->twine==1.9.0) (2024.8.30)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -andas (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: urllib3==1.21.1 in /usr/local/lib/python3.10/dist-packages (1.21.1)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -andas (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting pandas==2.1\n",
            "  Downloading pandas-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas==2.1) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.1) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.1) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas==2.1) (1.16.0)\n",
            "Downloading pandas-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution -andas (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.4.1\n",
            "    Uninstalling pandas-1.4.1:\n",
            "      Successfully uninstalled pandas-1.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.10.1 requires numba>=0.57, but you have numba 0.55.0 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.1.0 which is incompatible.\n",
            "mizani 0.13.0 requires pandas>=2.2.0, but you have pandas 2.1.0 which is incompatible.\n",
            "pandas-gbq 0.24.0 requires packaging>=22.0.0, but you have packaging 20.9 which is incompatible.\n",
            "plotnine 0.14.0 requires pandas>=2.2.0, but you have pandas 2.1.0 which is incompatible.\n",
            "shap 0.46.0 requires packaging>20.9, but you have packaging 20.9 which is incompatible.\n",
            "statsmodels 0.14.4 requires packaging>=21.3, but you have packaging 20.9 which is incompatible.\n",
            "statsmodels 0.14.4 requires pandas!=2.1.0,>=1.4, but you have pandas 2.1.0 which is incompatible.\n",
            "xarray 2024.10.0 requires packaging>=23.1, but you have packaging 20.9 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-2.1.0\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -andas (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting packaging==23.1\n",
            "  Downloading packaging-23.1-py3-none-any.whl.metadata (3.1 kB)\n",
            "Downloading packaging-23.1-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution -andas (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: packaging\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 20.9\n",
            "    Uninstalling packaging-20.9:\n",
            "      Successfully uninstalled packaging-20.9\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.10.1 requires numba>=0.57, but you have numba 0.55.0 which is incompatible.\n",
            "langchain-core 0.3.13 requires packaging<25,>=23.2, but you have packaging 23.1 which is incompatible.\n",
            "plotnine 0.14.0 requires pandas>=2.2.0, but you have pandas 2.1.0 which is incompatible.\n",
            "statsmodels 0.14.4 requires pandas!=2.1.0,>=1.4, but you have pandas 2.1.0 which is incompatible.\n",
            "tensorflowjs 3.18.0 requires packaging~=20.9, but you have packaging 23.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed packaging-23.1\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -andas (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting numpy==1.24.4\n",
            "  Using cached numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Using cached numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -andas (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.10.1 requires numba>=0.57, but you have numba 0.55.0 which is incompatible.\n",
            "mizani 0.13.0 requires pandas>=2.2.0, but you have pandas 2.1.0 which is incompatible.\n",
            "numba 0.55.0 requires numpy<1.22,>=1.18, but you have numpy 1.24.4 which is incompatible.\n",
            "plotnine 0.14.0 requires pandas>=2.2.0, but you have pandas 2.1.0 which is incompatible.\n",
            "rmm-cu12 24.10.0 requires numba>=0.57, but you have numba 0.55.0 which is incompatible.\n",
            "statsmodels 0.14.4 requires pandas!=2.1.0,>=1.4, but you have pandas 2.1.0 which is incompatible.\n",
            "tensorflowjs 3.18.0 requires packaging~=20.9, but you have packaging 23.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "606088c945ff4852b4c7e7fc33145d93"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -andas (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numba==0.55.0 in /usr/local/lib/python3.10/dist-packages (0.55.0)\n",
            "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /usr/local/lib/python3.10/dist-packages (from numba==0.55.0) (0.38.1)\n",
            "Collecting numpy<1.22,>=1.18 (from numba==0.55.0)\n",
            "  Using cached numpy-1.21.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba==0.55.0) (75.1.0)\n",
            "Using cached numpy-1.21.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -andas (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.24.4\n",
            "    Uninstalling numpy-1.24.4:\n",
            "      Successfully uninstalled numpy-1.24.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albucore 0.0.19 requires numpy>=1.24.4, but you have numpy 1.21.6 which is incompatible.\n",
            "albumentations 1.4.20 requires numpy>=1.24.4, but you have numpy 1.21.6 which is incompatible.\n",
            "arviz 0.20.0 requires numpy>=1.23.0, but you have numpy 1.21.6 which is incompatible.\n",
            "astropy 6.1.4 requires numpy>=1.23, but you have numpy 1.21.6 which is incompatible.\n",
            "bigframes 1.25.0 requires numpy>=1.24.0, but you have numpy 1.21.6 which is incompatible.\n",
            "chex 0.1.87 requires numpy>=1.24.1, but you have numpy 1.21.6 which is incompatible.\n",
            "contourpy 1.3.0 requires numpy>=1.23, but you have numpy 1.21.6 which is incompatible.\n",
            "cudf-cu12 24.10.1 requires numba>=0.57, but you have numba 0.55.0 which is incompatible.\n",
            "cudf-cu12 24.10.1 requires numpy<3.0a0,>=1.23, but you have numpy 1.21.6 which is incompatible.\n",
            "flax 0.8.5 requires numpy>=1.22, but you have numpy 1.21.6 which is incompatible.\n",
            "geopandas 1.0.1 requires numpy>=1.22, but you have numpy 1.21.6 which is incompatible.\n",
            "ibis-framework 9.2.0 requires numpy<3,>=1.23.2, but you have numpy 1.21.6 which is incompatible.\n",
            "jax 0.4.33 requires numpy>=1.24, but you have numpy 1.21.6 which is incompatible.\n",
            "jaxlib 0.4.33 requires numpy>=1.24, but you have numpy 1.21.6 which is incompatible.\n",
            "mizani 0.13.0 requires numpy>=1.23.5, but you have numpy 1.21.6 which is incompatible.\n",
            "mizani 0.13.0 requires pandas>=2.2.0, but you have pandas 2.1.0 which is incompatible.\n",
            "nibabel 5.3.2 requires numpy>=1.22, but you have numpy 1.21.6 which is incompatible.\n",
            "numexpr 2.10.1 requires numpy>=1.23.0, but you have numpy 1.21.6 which is incompatible.\n",
            "nx-cugraph-cu12 24.10.0 requires numpy<3.0a0,>=1.23, but you have numpy 1.21.6 which is incompatible.\n",
            "pandas 2.1.0 requires numpy>=1.22.4; python_version < \"3.11\", but you have numpy 1.21.6 which is incompatible.\n",
            "pandas-stubs 2.2.2.240909 requires numpy>=1.23.5, but you have numpy 1.21.6 which is incompatible.\n",
            "plotnine 0.14.0 requires numpy>=1.23.5, but you have numpy 1.21.6 which is incompatible.\n",
            "plotnine 0.14.0 requires pandas>=2.2.0, but you have pandas 2.1.0 which is incompatible.\n",
            "pylibraft-cu12 24.10.0 requires numpy<3.0a0,>=1.23, but you have numpy 1.21.6 which is incompatible.\n",
            "rmm-cu12 24.10.0 requires numba>=0.57, but you have numba 0.55.0 which is incompatible.\n",
            "rmm-cu12 24.10.0 requires numpy<3.0a0,>=1.23, but you have numpy 1.21.6 which is incompatible.\n",
            "scikit-image 0.24.0 requires numpy>=1.23, but you have numpy 1.21.6 which is incompatible.\n",
            "scipy 1.13.1 requires numpy<2.3,>=1.22.4, but you have numpy 1.21.6 which is incompatible.\n",
            "statsmodels 0.14.4 requires numpy<3,>=1.22.3, but you have numpy 1.21.6 which is incompatible.\n",
            "statsmodels 0.14.4 requires pandas!=2.1.0,>=1.4, but you have pandas 2.1.0 which is incompatible.\n",
            "tensorflow 2.17.0 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 1.21.6 which is incompatible.\n",
            "tensorflowjs 3.18.0 requires packaging~=20.9, but you have packaging 23.1 which is incompatible.\n",
            "tensorstore 0.1.67 requires numpy>=1.22.0, but you have numpy 1.21.6 which is incompatible.\n",
            "xarray 2024.10.0 requires numpy>=1.24, but you have numpy 1.21.6 which is incompatible.\n",
            "xarray-einstats 0.8.0 requires numpy>=1.23, but you have numpy 1.21.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.21.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "cab4d7c040bf420d88068d77b7b4b20e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -andas (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting tflite-model-maker==0.4.2\n",
            "  Using cached tflite_model_maker-0.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting tf-models-official==2.3.0 (from tflite-model-maker==0.4.2)\n",
            "  Using cached tf_models_official-2.3.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from tflite-model-maker==0.4.2) (1.21.6)\n",
            "Requirement already satisfied: pillow>=7.0.0 in /usr/local/lib/python3.10/dist-packages (from tflite-model-maker==0.4.2) (10.4.0)\n",
            "Requirement already satisfied: sentencepiece>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from tflite-model-maker==0.4.2) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-datasets>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from tflite-model-maker==0.4.2) (4.9.6)\n",
            "Collecting fire>=0.3.1 (from tflite-model-maker==0.4.2)\n",
            "  Using cached fire-0.7.0.tar.gz (87 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tflite-model-maker==0.4.2) (24.3.25)\n",
            "Requirement already satisfied: absl-py>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from tflite-model-maker==0.4.2) (1.4.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from tflite-model-maker==0.4.2) (1.21.1)\n",
            "Collecting tflite-support>=0.4.2 (from tflite-model-maker==0.4.2)\n",
            "  Downloading tflite_support-0.4.4-cp310-cp310-manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: tensorflowjs<3.19.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from tflite-model-maker==0.4.2) (3.18.0)\n",
            "Requirement already satisfied: tensorflow>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from tflite-model-maker==0.4.2) (2.17.0)\n",
            "INFO: pip is looking at multiple versions of tflite-model-maker to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Ignored the following versions that require a different python version: 0.52.0 Requires-Python >=3.6,<3.9; 0.52.0rc3 Requires-Python >=3.6,<3.9; 0.53.0 Requires-Python >=3.6,<3.10; 0.53.0rc1.post1 Requires-Python >=3.6,<3.10; 0.53.0rc2 Requires-Python >=3.6,<3.10; 0.53.0rc3 Requires-Python >=3.6,<3.10; 0.53.1 Requires-Python >=3.6,<3.10; 0.54.0 Requires-Python >=3.7,<3.10; 0.54.0rc2 Requires-Python >=3.7,<3.10; 0.54.0rc3 Requires-Python >=3.7,<3.10; 0.54.1 Requires-Python >=3.7,<3.10\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement numba==0.53 (from tflite-model-maker) (from versions: 0.1, 0.2, 0.3, 0.5.0, 0.6.0, 0.7.0, 0.7.1, 0.7.2, 0.8.0, 0.8.1, 0.9.0, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.12.1, 0.12.2, 0.13.0, 0.13.2, 0.13.3, 0.13.4, 0.14.0, 0.15.1, 0.16.0, 0.17.0, 0.18.1, 0.18.2, 0.19.1, 0.19.2, 0.20.0, 0.21.0, 0.22.0, 0.22.1, 0.23.0, 0.23.1, 0.24.0, 0.25.0, 0.26.0, 0.27.0, 0.28.1, 0.29.0, 0.30.0, 0.30.1, 0.31.0, 0.32.0, 0.33.0, 0.34.0, 0.35.0, 0.36.1, 0.36.2, 0.37.0, 0.38.0, 0.38.1, 0.39.0, 0.40.0, 0.40.1, 0.41.0, 0.42.0, 0.42.1, 0.43.0, 0.43.1, 0.44.0, 0.44.1, 0.45.0, 0.45.1, 0.46.0, 0.47.0, 0.48.0, 0.49.0, 0.49.1rc1, 0.49.1, 0.50.0rc1, 0.50.0, 0.50.1, 0.51.0rc1, 0.51.0, 0.51.1, 0.51.2, 0.52.0rc2, 0.55.0rc1, 0.55.0, 0.55.1, 0.55.2, 0.56.0rc1, 0.56.0, 0.56.2, 0.56.3, 0.56.4, 0.57.0rc1, 0.57.0, 0.57.1rc1, 0.57.1, 0.58.0rc1, 0.58.0rc2, 0.58.0, 0.58.1, 0.59.0rc1, 0.59.0, 0.59.1, 0.60.0rc1, 0.60.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for numba==0.53\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: tf2onnx in /usr/local/lib/python3.10/dist-packages (1.16.1)\n",
            "Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (1.21.6)\n",
            "Requirement already satisfied: onnx>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (1.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (1.16.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (24.3.25)\n",
            "Requirement already satisfied: protobuf~=3.20 in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (3.20.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (1.21.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (2024.8.30)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -andas (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-model-optimization==0.7.0\n",
        "!pip install tensorflowjs==3.18.0\n",
        "!pip install twine==1.9.0\n",
        "!pip install urllib3==1.21.1\n",
        "!pip install pandas==1.4.1\n",
        "!pip install numpy==1.23.5\n",
        "!pip install numba==0.53\n",
        "!pip install tflite-model-maker==0.4.2\n",
        "!pip install tf2onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ek9ev_GoEsH"
      },
      "source": [
        "### Step2. Load and Export Model Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK98eSNLrVmI"
      },
      "source": [
        "Download and extract the sample dataset for object detection, which compatible format is:\n",
        "  * [Pascal_Voc](https://itriaihub.blob.core.windows.net/datasets/Detect_HardHat_244x244.pascal_voc.tar.xz)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://itriaihub.blob.core.windows.net/github-download-resources/repository/ITRI-AI-Hub/datasets/HardHat_Dataset.PascalVOC.zip\n",
        "!unzip HardHat_Dataset.PascalVOC.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSAGvivKoz70",
        "outputId": "0f8a76d7-0591-41f1-8ae7-83eca37f00ae"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-06 08:44:19--  https://itriaihub.blob.core.windows.net/github-download-resources/repository/ITRI-AI-Hub/datasets/HardHat_Dataset.PascalVOC.zip\n",
            "Resolving itriaihub.blob.core.windows.net (itriaihub.blob.core.windows.net)... 20.209.42.36\n",
            "Connecting to itriaihub.blob.core.windows.net (itriaihub.blob.core.windows.net)|20.209.42.36|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3728463 (3.6M) [application/x-zip-compressed]\n",
            "Saving to: ‘HardHat_Dataset.PascalVOC.zip’\n",
            "\n",
            "HardHat_Dataset.Pas 100%[===================>]   3.56M  1.23MB/s    in 2.9s    \n",
            "\n",
            "2024-11-06 08:44:23 (1.23 MB/s) - ‘HardHat_Dataset.PascalVOC.zip’ saved [3728463/3728463]\n",
            "\n",
            "Archive:  HardHat_Dataset.PascalVOC.zip\n",
            "   creating: HardHat_Dataset.PascalVOC/\n",
            "  inflating: HardHat_Dataset.PascalVOC/README.roboflow.txt  \n",
            "   creating: HardHat_Dataset.PascalVOC/test/\n",
            "  inflating: HardHat_Dataset.PascalVOC/test/000008_jpg.rf.8ddEpaD8kjf9Wwgm1P3I.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/test/000008_jpg.rf.8ddEpaD8kjf9Wwgm1P3I.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/test/000011_jpg.rf.n6TiaiPM2lzuMoiSQNmo.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/test/000011_jpg.rf.n6TiaiPM2lzuMoiSQNmo.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/test/000034_jpg.rf.qntmyCFyZO1mm5K83Q2U.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/test/000034_jpg.rf.qntmyCFyZO1mm5K83Q2U.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/test/000047_jpg.rf.rrQnzhne8qC0aY5YFqTs.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/test/000047_jpg.rf.rrQnzhne8qC0aY5YFqTs.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/test/000054_jpg.rf.4rv9qXIezByFltihCAki.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/test/000054_jpg.rf.4rv9qXIezByFltihCAki.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/test/000073_jpg.rf.WaR1Cr7x6TS2HKvYK1x9.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/test/000073_jpg.rf.WaR1Cr7x6TS2HKvYK1x9.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/test/000076_jpg.rf.ySqZfAFjerLFbsr3lYFA.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/test/000076_jpg.rf.ySqZfAFjerLFbsr3lYFA.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/test/000084_jpg.rf.ypBrJPcN7GVM2jxfozBk.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/test/000084_jpg.rf.ypBrJPcN7GVM2jxfozBk.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/test/000097_jpg.rf.TiWcbiGnH5DmEu4s1ytr.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/test/000097_jpg.rf.TiWcbiGnH5DmEu4s1ytr.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/test/000098_jpg.rf.nnm9JVksIPPAqpQTx76U.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/test/000098_jpg.rf.nnm9JVksIPPAqpQTx76U.xml  \n",
            "   creating: HardHat_Dataset.PascalVOC/train/\n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000001_jpg.rf.lr4ZmXF3PvLg6puT9eTZ.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000001_jpg.rf.lr4ZmXF3PvLg6puT9eTZ.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000007_jpg.rf.mn7CnVZFyc8sfVSuTUU9.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000007_jpg.rf.mn7CnVZFyc8sfVSuTUU9.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000012_jpg.rf.SXG83CiV3friXnUAZFvn.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000012_jpg.rf.SXG83CiV3friXnUAZFvn.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000013_jpg.rf.NKiMm27JcltSIR4TH6Yw.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000013_jpg.rf.NKiMm27JcltSIR4TH6Yw.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000014_jpg.rf.Pl5PKx0zQyRw6Z4twhRh.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000014_jpg.rf.Pl5PKx0zQyRw6Z4twhRh.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000016_jpg.rf.ZIq2bpsCTJ3aoHXhMujn.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000016_jpg.rf.ZIq2bpsCTJ3aoHXhMujn.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000017_jpg.rf.fmnhbILLEYBAbbgkCUzY.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000017_jpg.rf.fmnhbILLEYBAbbgkCUzY.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000018_jpg.rf.ejU3o24j3VaxiLXoYfmm.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000018_jpg.rf.ejU3o24j3VaxiLXoYfmm.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000019_jpg.rf.7JZ1deu9Lr71Mr87XEyr.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000019_jpg.rf.7JZ1deu9Lr71Mr87XEyr.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000020_jpg.rf.6iWFXgOzff5GFKaCffKu.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000020_jpg.rf.6iWFXgOzff5GFKaCffKu.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000021_jpg.rf.GHFXMr2bYhIOcHTR2I8a.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000021_jpg.rf.GHFXMr2bYhIOcHTR2I8a.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000022_jpg.rf.IGCXkU0mFl300eqfrq4W.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000022_jpg.rf.IGCXkU0mFl300eqfrq4W.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000023_jpg.rf.2PtGhFk0VisqQG0UKgeS.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000023_jpg.rf.2PtGhFk0VisqQG0UKgeS.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000024_jpg.rf.GeuqSF7poGvwFxLyQnD0.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000024_jpg.rf.GeuqSF7poGvwFxLyQnD0.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000025_jpg.rf.j90EdnFwguNExJ8Qlnac.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000025_jpg.rf.j90EdnFwguNExJ8Qlnac.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000026_jpg.rf.lLox0IncnLd5vvl97wpt.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000026_jpg.rf.lLox0IncnLd5vvl97wpt.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000027_jpg.rf.VPRIBHENkqF8DRtsYoq6.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000027_jpg.rf.VPRIBHENkqF8DRtsYoq6.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000028_jpg.rf.cVcf9mxnqav9lIKjhKTB.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000028_jpg.rf.cVcf9mxnqav9lIKjhKTB.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000029_jpg.rf.EBQCCwqNWzhNSsO316mB.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000029_jpg.rf.EBQCCwqNWzhNSsO316mB.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000030_jpg.rf.norcz4nZ6jiSQSahqSSv.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000030_jpg.rf.norcz4nZ6jiSQSahqSSv.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000031_jpg.rf.0gOBdTRlsXHxeEoU6MrY.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000031_jpg.rf.0gOBdTRlsXHxeEoU6MrY.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000032_jpg.rf.cUdwomewOltL7m7H8tom.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000032_jpg.rf.cUdwomewOltL7m7H8tom.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000033_jpg.rf.jlc79Y5MRxqPQa92MJ2L.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000033_jpg.rf.jlc79Y5MRxqPQa92MJ2L.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000035_jpg.rf.b964v8uDAWCqCWr06243.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000035_jpg.rf.b964v8uDAWCqCWr06243.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000036_jpg.rf.Qnva2FdBk3VJ9TfcLnVE.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000036_jpg.rf.Qnva2FdBk3VJ9TfcLnVE.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000037_jpg.rf.5T7nwKm9LinkkoqsPvnQ.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000037_jpg.rf.5T7nwKm9LinkkoqsPvnQ.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000038_jpg.rf.blyuB6O3jcicHh6rcW6y.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000038_jpg.rf.blyuB6O3jcicHh6rcW6y.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000039_jpg.rf.DdAwtHEjNjAlF7MHMERy.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000039_jpg.rf.DdAwtHEjNjAlF7MHMERy.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000040_jpg.rf.Z0b27laDa9tM1udrtbxu.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000040_jpg.rf.Z0b27laDa9tM1udrtbxu.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000041_jpg.rf.ID0EFYpJ3AFVZ6dPexlu.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000041_jpg.rf.ID0EFYpJ3AFVZ6dPexlu.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000042_jpg.rf.gu1tKBJoYUPEK2TfmRgH.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000042_jpg.rf.gu1tKBJoYUPEK2TfmRgH.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000044_jpg.rf.Y7aNPeRVl953G0Smpk7B.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000044_jpg.rf.Y7aNPeRVl953G0Smpk7B.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000045_jpg.rf.63JtErP9ClkjFvImok08.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000045_jpg.rf.63JtErP9ClkjFvImok08.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000049_jpg.rf.NJsuzQfhycmkMUHo4XKk.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000049_jpg.rf.NJsuzQfhycmkMUHo4XKk.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000050_jpg.rf.s0QUkANVPyRBszh7UaS5.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000050_jpg.rf.s0QUkANVPyRBszh7UaS5.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000051_jpg.rf.Y1MHTGe7lgKqGKdeSvAA.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000051_jpg.rf.Y1MHTGe7lgKqGKdeSvAA.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000052_jpg.rf.bG12OWcn4wNqKupMqPVU.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000052_jpg.rf.bG12OWcn4wNqKupMqPVU.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000053_jpg.rf.42lW2wsPitDMMdIdeHeX.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000053_jpg.rf.42lW2wsPitDMMdIdeHeX.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000056_jpg.rf.DBLCiTl8d0xE20SR0nCN.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000056_jpg.rf.DBLCiTl8d0xE20SR0nCN.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000058_jpg.rf.YuZxHJr1b46TrpirGUGm.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000058_jpg.rf.YuZxHJr1b46TrpirGUGm.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000059_jpg.rf.b2F46aXvPxUGRWbxbaYA.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000059_jpg.rf.b2F46aXvPxUGRWbxbaYA.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000060_jpg.rf.neA0AtXF901C9mcPC3rg.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000060_jpg.rf.neA0AtXF901C9mcPC3rg.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000061_jpg.rf.yrzgWg4swwYtUN4zfaev.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000061_jpg.rf.yrzgWg4swwYtUN4zfaev.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000062_jpg.rf.qaBCRl1lJcfnE3dZHuNB.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000062_jpg.rf.qaBCRl1lJcfnE3dZHuNB.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000064_jpg.rf.rZXmv7lDI6bSMRfV9pzc.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000064_jpg.rf.rZXmv7lDI6bSMRfV9pzc.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000066_jpg.rf.n42Z6WlJzId2bXIyTocO.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000066_jpg.rf.n42Z6WlJzId2bXIyTocO.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000067_jpg.rf.5QXCAaj4ORQgu79PXmF5.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000067_jpg.rf.5QXCAaj4ORQgu79PXmF5.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000068_jpg.rf.tonB2bT09Vu1Wj6hoCnQ.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000068_jpg.rf.tonB2bT09Vu1Wj6hoCnQ.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000069_jpg.rf.d1MM9KbeNiI7Rk11xq91.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000069_jpg.rf.d1MM9KbeNiI7Rk11xq91.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000070_jpg.rf.FS0D4Yn9tGvqhqV04Bqw.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000070_jpg.rf.FS0D4Yn9tGvqhqV04Bqw.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000072_jpg.rf.9sjrqItGLF1muAnUsp4X.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000072_jpg.rf.9sjrqItGLF1muAnUsp4X.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000074_jpg.rf.0mhNgaIhuK6AksOZuIE7.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000074_jpg.rf.0mhNgaIhuK6AksOZuIE7.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000075_jpg.rf.JtQTSb5UCbP3592TkThM.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000075_jpg.rf.JtQTSb5UCbP3592TkThM.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000077_jpg.rf.9n4RChdCQApHrjx3LZz1.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000077_jpg.rf.9n4RChdCQApHrjx3LZz1.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000078_jpg.rf.H2ek4btF7N2ghdmXFv8i.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000078_jpg.rf.H2ek4btF7N2ghdmXFv8i.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000079_jpg.rf.IOebpXGwlLZVvC0FcPt3.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000079_jpg.rf.IOebpXGwlLZVvC0FcPt3.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000080_jpg.rf.ccWXnwgMKKHp9BYA3ARq.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000080_jpg.rf.ccWXnwgMKKHp9BYA3ARq.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000081_jpg.rf.ZcWyAWeqDpUm1Bpmpk3W.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000081_jpg.rf.ZcWyAWeqDpUm1Bpmpk3W.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000082_jpg.rf.imhAb6oKepLm9B3TA9q0.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000082_jpg.rf.imhAb6oKepLm9B3TA9q0.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000083_jpg.rf.3kqxcVBX5xiUn1seP3UR.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000083_jpg.rf.3kqxcVBX5xiUn1seP3UR.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000085_jpg.rf.aZ3Q7Wy94WQ6i05e7CM0.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000085_jpg.rf.aZ3Q7Wy94WQ6i05e7CM0.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000086_jpg.rf.vOm6zJIiagrPWUcDWZT2.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000086_jpg.rf.vOm6zJIiagrPWUcDWZT2.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000087_jpg.rf.lJPmeH4TPXtfnCZVJrGY.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000087_jpg.rf.lJPmeH4TPXtfnCZVJrGY.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000089_jpg.rf.wvu4hEGMA72ZcIV5niww.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000089_jpg.rf.wvu4hEGMA72ZcIV5niww.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000090_jpg.rf.thD0aBzhv7T3yHKyHpL9.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000090_jpg.rf.thD0aBzhv7T3yHKyHpL9.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000092_jpg.rf.rb0MN0wMcwaLhvNz8AFx.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000092_jpg.rf.rb0MN0wMcwaLhvNz8AFx.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000093_jpg.rf.vyRp7GG7PHuksTTLWHpj.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000093_jpg.rf.vyRp7GG7PHuksTTLWHpj.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000094_jpg.rf.DpdZYGvWVeZZdz79mEuX.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000094_jpg.rf.DpdZYGvWVeZZdz79mEuX.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000096_jpg.rf.dRa1uZXefKajmoBDBLCm.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000096_jpg.rf.dRa1uZXefKajmoBDBLCm.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000100_jpg.rf.BzOp5ivVZTzPYFCmYYdx.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/train/000100_jpg.rf.BzOp5ivVZTzPYFCmYYdx.xml  \n",
            "   creating: HardHat_Dataset.PascalVOC/valid/\n",
            "  inflating: HardHat_Dataset.PascalVOC/valid/000002_jpg.rf.TlCjYNxN0dHXRn6ctWn5.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/valid/000002_jpg.rf.TlCjYNxN0dHXRn6ctWn5.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/valid/000003_jpg.rf.EjIdAhHuafGrfVhehoij.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/valid/000003_jpg.rf.EjIdAhHuafGrfVhehoij.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/valid/000004_jpg.rf.2GdL1eNABqI43qeB9rXr.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/valid/000004_jpg.rf.2GdL1eNABqI43qeB9rXr.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/valid/000005_jpg.rf.IK4SxvmKAdE1oZQ2HTDP.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/valid/000005_jpg.rf.IK4SxvmKAdE1oZQ2HTDP.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/valid/000006_jpg.rf.flfDLCVd3tPH0v65BrQC.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/valid/000006_jpg.rf.flfDLCVd3tPH0v65BrQC.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/valid/000009_jpg.rf.Ir5w16tzcvVT0LmMaLsj.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/valid/000009_jpg.rf.Ir5w16tzcvVT0LmMaLsj.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/valid/000010_jpg.rf.7zS8QbkzXx5yCyeU2AuQ.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/valid/000010_jpg.rf.7zS8QbkzXx5yCyeU2AuQ.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/valid/000015_jpg.rf.tRd4zoJP3vdWLz8IH3gN.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/valid/000015_jpg.rf.tRd4zoJP3vdWLz8IH3gN.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/valid/000043_jpg.rf.QmNTLAsIrQ7QLD1t6Oue.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/valid/000043_jpg.rf.QmNTLAsIrQ7QLD1t6Oue.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/valid/000046_jpg.rf.kKFomGyV5pD4rFt0cp6t.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/valid/000046_jpg.rf.kKFomGyV5pD4rFt0cp6t.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/valid/000048_jpg.rf.GvZcBmbGjfHpZ7oZ9k2n.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/valid/000048_jpg.rf.GvZcBmbGjfHpZ7oZ9k2n.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/valid/000055_jpg.rf.0Yk2MA7lmPmTcczN0Tcr.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/valid/000055_jpg.rf.0Yk2MA7lmPmTcczN0Tcr.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/valid/000063_jpg.rf.fv1FHLDZzHuTGcFh2NHk.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/valid/000063_jpg.rf.fv1FHLDZzHuTGcFh2NHk.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/valid/000065_jpg.rf.FX2AuBNCGfGzs94x3kmD.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/valid/000065_jpg.rf.FX2AuBNCGfGzs94x3kmD.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/valid/000071_jpg.rf.waLD6hhz137pK57LrHgW.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/valid/000071_jpg.rf.waLD6hhz137pK57LrHgW.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/valid/000088_jpg.rf.B6UqpphUZFXwTPdQgrNQ.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/valid/000088_jpg.rf.B6UqpphUZFXwTPdQgrNQ.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/valid/000091_jpg.rf.LrtkMqnIntdJ843SL5ax.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/valid/000091_jpg.rf.LrtkMqnIntdJ843SL5ax.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/valid/000099_jpg.rf.HDeWNLsEC4a7Lp3dgBrH.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/valid/000099_jpg.rf.HDeWNLsEC4a7Lp3dgBrH.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/valid/000101_jpg.rf.onyq0ctZMDwxamdFdycU.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/valid/000101_jpg.rf.onyq0ctZMDwxamdFdycU.xml  \n",
            "  inflating: HardHat_Dataset.PascalVOC/valid/000102_jpg.rf.NxJRm40Pcb42W5gbi1S7.jpg  \n",
            "  inflating: HardHat_Dataset.PascalVOC/valid/000102_jpg.rf.NxJRm40Pcb42W5gbi1S7.xml  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yC7o9BfyrTeC",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "73d64e3b-362c-4d15-cad8-822880eb6c61"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tflite_model_maker'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-0e5a5340f164>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtflite_model_maker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mobject_detector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mDATASET_FOLDER_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Detect_HardHat_244x244.pascal_voc'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mCLASSES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{DATASET_FOLDER_PATH}/data.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tflite_model_maker'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from tflite_model_maker import object_detector\n",
        "import numpy as np\n",
        "\n",
        "DATASET_FOLDER_PATH = '/content/HardHat_Dataset.PascalVOC'\n",
        "#CLASSES = np.load(f'{DATASET_FOLDER_PATH}/data.npy').tolist()\n",
        "\n",
        "train_data = object_detector.DataLoader.from_pascal_voc(f'{DATASET_FOLDER_PATH}/train', f'{DATASET_FOLDER_PATH}/train', CLASSES)\n",
        "valid_data = object_detector.DataLoader.from_pascal_voc(f'{DATASET_FOLDER_PATH}/test', f'{DATASET_FOLDER_PATH}/test', CLASSES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CVdbC5uoNCk"
      },
      "source": [
        "Build Model with Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYKNtEVJqrfc",
        "scrolled": true,
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "from tflite_model_maker import model_spec\n",
        "import tensorflow as tf\n",
        "\n",
        "BACKBONE = 'efficientdet_lite4' # @param [\"efficientdet_lite0\", \"efficientdet_lite1\", \"efficientdet_lite2\", \"efficientdet_lite3\", \"efficientdet_lite4\"]\n",
        "\n",
        "# Pre-train is necessary in AutoML Framework\n",
        "model = object_detector.create(train_data, model_spec=model_spec.get(BACKBONE), epochs=1, batch_size=8,\n",
        "                               train_whole_model=True, validation_data=valid_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1I7PpHWous1"
      },
      "source": [
        "We can preview `Params` of this model via Summary() and fill it to ITRI-AI-Hub*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-d_bna6Hous1"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzT0zx5Rs3Ze"
      },
      "source": [
        "3. Export the model and upload to [ITRI Azure Instance]() and link to [Mode Zoo]()."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8W2gDe6Lous2"
      },
      "source": [
        "* **Format 1**: TFLite\n",
        "(*We can preview `GFLOPS` of this model via console outputs and fill it to ITRI-AI-Hub**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "chpuoAYxous2"
      },
      "outputs": [],
      "source": [
        "from tflite_model_maker.config import ExportFormat\n",
        "\n",
        "model.export(export_dir=f'{BACKBONE}', export_format=[ExportFormat.TFLITE])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgjrgW0gous2"
      },
      "source": [
        "* **Format 2**: ONNX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Oq1GCiiZous3"
      },
      "outputs": [],
      "source": [
        "!python -m tf2onnx.convert --opset 16 --tflite {BACKBONE}/model.tflite --output {BACKBONE}/model.onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgxenwbTous3"
      },
      "source": [
        "### Output differents after export (*not prepared yet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWKlaZBcsED6"
      },
      "source": [
        "### Benchmarking\n",
        "> * You can request access to `model_zo_library` from your ITRI-AI-Hub collaborator and port the `model` and `model_zoo_library` on your device to implement the standardised test procedure.\n",
        ">   \n",
        ">   ```bash\n",
        ">   git clone https://github.com/R300-AI/model_zoo_library.git\n",
        ">   cd model_zoo_library\n",
        ">\n",
        ">   python3 profile.py --model <path-to-model>.tflite --platform <HW_series_name> --chipset <processor_type>\n",
        ">   ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgLQTJoNous3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}